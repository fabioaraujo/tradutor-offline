# Tradutor Ingl√™s para Portugu√™s - API OpenAI Local

Tradutor de arquivos de texto de ingl√™s para portugu√™s usando API OpenAI compat√≠vel (LM Studio, Ollama, LocalAI, etc.) com modelos de linguagem locais.

## Caracter√≠sticas

- ‚úÖ **Flex√≠vel**: Funciona com qualquer modelo compat√≠vel com API OpenAI
- ‚úÖ **Precis√£o Linha por Linha**: Traduz cada linha individualmente para m√°xima fidelidade
- ‚úÖ **Servidor Local**: Suporte para LM Studio, Ollama, LocalAI, text-generation-webui
- ‚úÖ **Monitoramento Detalhado**: Barra de progresso com velocidade, ETA e estat√≠sticas em tempo real
- ‚úÖ **Configur√°vel**: API URL, modelo e tamanho de lote customiz√°veis
- ‚úÖ **Alta Qualidade**: 95-98% de acur√°cia com modelos como Qwen 2.5 7B
- ‚úÖ **Performance Consistente**: ~4.5 linhas/segundo (testado com 1577 linhas)

### Utilit√°rio Adicional
- ‚úÖ **juntar_linhas.py**: Junta senten√ßas quebradas em m√∫ltiplas linhas

## Requisitos

- Python 3.9+
- uv (gerenciador de pacotes Python)
- Servidor com API OpenAI compat√≠vel (LM Studio, Ollama, etc.)
- Modelo de linguagem carregado no servidor

## Instala√ß√£o

1. Instale o uv (se ainda n√£o tiver):
```bash
pip install uv
```

2. Instale as depend√™ncias do projeto:
```bash
uv sync
```

As √∫nicas depend√™ncias s√£o:
- **requests**: Para requisi√ß√µes HTTP √† API
- Biblioteca padr√£o do Python
- NVIDIA GPU com suporte CUDA (8GB+ VRAM recomendado)
- Drivers NVIDIA atualizados
- CUDA 12.1 ou superior

## Uso

### Op√ß√£o 1: Tradutor mBART-50 (Offline, GPU/CPU)

#### Traduzir o arquivo texto.txt (padr√£o)

```bash
uv run tradutor.py
```

Isso ir√° traduzir o arquivo `texto.txt` e criar `texto_traduzido.txt` com a tradu√ß√£o.

#### Traduzir arquivo customizado

```bash
uv run tradutor.py arquivo_entrada.txt arquivo_saida.txt
## Uso

### Tradutor OpenAI API (Servidor Local)

#### Uso b√°sico (com LM Studio ou similar rodando em localhost:1234)

```bash
uv run tradutor_openai.py
```

#### Uso com par√¢metros customizados

```bash
# Sintaxe: python tradutor_openai.py [entrada] [saida] [api_url] [modelo] [linhas_por_lote]
uv run tradutor_openai.py texto.txt saida.txt http://127.0.0.1:1234/v1 local-model 5
```

**Par√¢metros:**
- `entrada`: Arquivo de entrada (padr√£o: texto.txt)
- `saida`: Arquivo de sa√≠da (padr√£o: texto_traduzido.txt)
- `api_url`: URL base da API OpenAI (padr√£o: http://127.0.0.1:1234/v1)
- `modelo`: Nome do modelo (padr√£o: local-model)
- `linhas_por_lote`: N√∫mero de linhas por requisi√ß√£o (padr√£o: 5)

**Servidores compat√≠veis:**
- **LM Studio** (Recomendado)
- Ollama (com endpoint OpenAI)
- LocalAI
- text-generation-webui (com extens√£o OpenAI)
- Qualquer servidor compat√≠vel com API OpenAI

### Utilit√°rio para Juntar Linhas

√ötil para arquivos de legendas ou texto quebrado em m√∫ltiplas linhas:

```bash
uv run juntar_linhas.py entrada.txt saida.txt
```

**Recursos:**
- Junta senten√ßas quebradas mantendo estrutura
- Preserva par√°grafos e linhas vazias
- Detecta automaticamente continua√ß√£o de frases
- Mant√©m pontua√ß√£o e formata√ß√£o

## Como funciona

O tradutor_openai.py se conecta a um servidor local (como LM Studio) que executa modelos de linguagem grandes (LLMs) otimizados via API compat√≠vel com OpenAI.

**Funcionamento:**
1. L√™ o arquivo de entrada linha por linha
2. Agrupa linhas em lotes (padr√£o: 5 linhas)
3. Envia cada linha individualmente para tradu√ß√£o via API
4. Recebe a tradu√ß√£o e mant√©m a estrutura original
5. Salva o resultado preservando linhas vazias e formata√ß√£o

**Vantagens:**
- ‚úÖ Tradu√ß√£o linha por linha para m√°xima precis√£o
- ‚úÖ N√£o requer instala√ß√£o de modelos pesados (gerenciados pelo servidor)
- ‚úÖ Flex√≠vel: troque de modelo facilmente no servidor
- ‚úÖ Barra de progresso com estat√≠sticas em tempo real
- ‚úÖ Performance consistente e previs√≠vel

## Modelos Recomendados

### üéØ Guia de Sele√ß√£o de Modelos para LM Studio

Para obter qualidade de tradu√ß√£o equivalente ou superior ao mBART-50 (95%+):

#### 1. **Qwen 2.5 7B Instruct** ‚≠ê Recomendado
```
Modelo: bartowski/Qwen2.5-7B-Instruct-GGUF
Quantiza√ß√£o: Q6_K (melhor qualidade) ou Q5_K_M (equilibrado)
```
- ‚úÖ **Excelente em multil√≠ngue**: Treinado em 29 idiomas incluindo portugu√™s
- ‚úÖ **Qualidade**: 95-98% (superior ao mBART)
- ‚úÖ **Velocidade**: ~4.5 linhas/s
- ‚úÖ **VRAM**: ~8GB (Q6) ou 6-7GB (Q5)
- ‚úÖ **Tradu√ß√£o natural**: Melhor contexto que mBART

#### 2. **Llama 3.1 8B Instruct**
```
Modelo: bartowski/Meta-Llama-3.1-8B-Instruct-GGUF
Quantiza√ß√£o: Q5_K_M ou Q6_K
```
- ‚úÖ **Muito popular**: Amplamente testado
- ‚úÖ **Qualidade**: 90-95%
- ‚úÖ **Velocidade**: ~4-6 linhas/s
- ‚úÖ **VRAM**: ~6-8GB

#### 3. **Aya 23 8B** (Especializado Multil√≠ngue)
```
Modelo: CohereForAI/aya-23-8B-GGUF
Quantiza√ß√£o: Q5_K_M
```
- ‚úÖ **Especializado**: Focado em 23 idiomas incluindo portugu√™s
- ‚úÖ **Qualidade**: 95%+
- ‚úÖ **VRAM**: ~6-8GB

#### 4. **Mixtral 8x7B** (M√°xima Qualidade)
```
Modelo: TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF
Quantiza√ß√£o: Q4_K_M ou Q5_K_M
```
- ‚úÖ **Qualidade excepcional**: 98%+
- ‚úÖ **Contexto superior**: 32k tokens
- ‚ö†Ô∏è **VRAM**: ~12GB (usa GPU completa)
- ‚ö†Ô∏è **Mais lento**: ~2-3 linhas/s

## Performance

**Hardware de refer√™ncia**: RTX 4070 Ti (12GB VRAM) - LM Studio rodando localmente

| Modelo | Quantiza√ß√£o | Velocidade | Tempo (1577 linhas) | Tempo est. (2780 linhas) | Qualidade | VRAM |
|--------|-------------|------------|---------------------|--------------------------|-----------|------|
| **Qwen 2.5 7B** ‚≠ê | Q6_K | ~4.5 linhas/s | ~5m 47s | ~10m 17s | 95-98% | 8GB |
| **Llama 3.1 8B** | Q5_K_M | ~4-6 linhas/s | ~4-7 min | ~8-12 min | 90-95% | 7GB |
| **Aya 23 8B** | Q5_K_M | ~3-5 linhas/s | ~5-9 min | ~10-15 min | 95%+ | 7GB |
| **Mixtral 8x7B** | Q4_K_M | ~2-3 linhas/s | ~9-13 min | ~15-25 min | 98%+ | 12GB |
| **GPT-4 (Cloud)** | - | Vari√°vel | Vari√°vel | Depende da API | 98%+ | N/A |

**Testes Reais Executados**:
1. **Arquivo pequeno (29 linhas)**: 6.48s = 4.48 l/s
2. **Arquivo grande (1577 linhas)**: 347.13s (5m 47s) = **4.54 l/s** ‚úÖ
   - Modelo: Qwen 2.5 7B (Q6_K) no LM Studio
   - Hardware: RTX 4070 Ti (12GB VRAM)
   - Configura√ß√£o: 5 linhas por lote, temperature 0.3
   - Performance consistente em arquivos de diferentes tamanhos

**Nota**: 
- Performance medida com temperatura 0.3 e 5 linhas por lote
- Velocidade muito consistente entre arquivos pequenos e grandes
- LM Studio, Ollama e LocalAI t√™m performance similar
- Modelos GGUF (quantizados) oferecem melhor velocidade/qualidade
- Para 2780 linhas: ~10 minutos estimados com Qwen 2.5 7B

## Solu√ß√£o de Problemas

### Problemas com OpenAI API (tradutor_openai.py)

#### Erro de conex√£o com a API
```bash
# Verifique se o servidor est√° rodando
curl http://127.0.0.1:1234/v1/models

# Ou no PowerShell
Invoke-WebRequest http://127.0.0.1:1234/v1/models
```

#### API retorna erros 500/502
- Verifique se o modelo est√° carregado no servidor
- Confirme que o nome do modelo est√° correto
- Verifique logs do servidor (LM Studio, Ollama, etc.)

#### Tradu√ß√£o muito lenta
- Reduza `linhas_por_lote` para 1-3 linhas
- Use modelo menor e mais r√°pido
- Verifique se o servidor est√° usando GPU

#### Tradu√ß√£o de baixa qualidade
- Use modelos maiores (Mixtral, Llama 3.1 70B, etc.)
- Aumente a `temperature` no c√≥digo (padr√£o: 0.3)
- Teste diferentes prompts no c√≥digo

#### Erro "requests module not found"
```bash
uv pip install requests
```

## Hist√≥rico de Desenvolvimento

### v3.0 (Atual) - API OpenAI
- ‚úÖ Tradutor otimizado com API OpenAI local (tradutor_openai.py)
- ‚úÖ Suporte para LM Studio, Ollama e outros servidores
- ‚úÖ Tradu√ß√£o linha por linha com alta precis√£o (95-98%)
- ‚úÖ Performance consistente: ~4.5 linhas/s com Qwen 2.5 7B
- ‚úÖ Utilit√°rio para juntar linhas quebradas (juntar_linhas.py)
- ‚úÖ Configura√ß√£o flex√≠vel de API, modelo e batch size
- ‚úÖ Barra de progresso com estat√≠sticas em tempo real
- ‚úÖ Testado em produ√ß√£o com arquivos de 1577+ linhas

## Licen√ßa

MIT License - C√≥digo livre para uso pessoal e comercial.
